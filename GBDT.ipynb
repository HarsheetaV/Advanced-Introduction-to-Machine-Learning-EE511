{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from multiprocessing import Pool\n",
    "#from functools import partial\n",
    "import numpy as np\n",
    "#from numba import jit\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from pdb import set_trace as bp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: loss of least square regression and binary logistic regression\n",
    "'''\n",
    "    pred() takes GBDT/RF outputs, i.e., the \"score\", as its inputs, and returns predictions.\n",
    "    g() is the gradient/1st order derivative, which takes true values \"true\" and scores as input, and returns gradient.\n",
    "    h() is the heassian/2nd order derivative, which takes true values \"true\" and scores as input, and returns hessian.\n",
    "'''\n",
    "class leastsquare(object):\n",
    "    '''Loss class for mse. As for mse, pred function is pred=score.'''\n",
    "    def pred(self,score):\n",
    "        return score\n",
    "\n",
    "    def g(self,true,score):\n",
    "        return -2*(true - score)\n",
    "\n",
    "    def h(self,true,score):\n",
    "        return 2*np.ones_like(true)\n",
    "\n",
    "class logistic(object):\n",
    "    '''Loss class for log loss. As for log loss, pred function is logistic transformation.'''\n",
    "    def pred(self,score):\n",
    "        return np.round(1 / (1 + np.exp(-np.array(score))))\n",
    "\n",
    "    def g(self,true,score):\n",
    "        return -((true-1)*np.exp(score)+true) / (np.exp(score)+1)\n",
    "\n",
    "    def h(self,true,score):\n",
    "        return np.exp(score) / ((np.exp(score)+1)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: class of Random Forest\n",
    "class RF(object):\n",
    "    '''\n",
    "    Class of Random Forest\n",
    "    \n",
    "    Parameters:\n",
    "        n_threads: The number of threads used for fitting and predicting.\n",
    "        loss: Loss function for gradient boosting.\n",
    "            'mse' for regression task and 'log' for classfication task.\n",
    "            A child class of the loss class could be passed to implement customized loss.\n",
    "        max_depth: The maximum depth d_max of a tree.\n",
    "        min_sample_split: The minimum number of samples required to further split a node.\n",
    "        lamda: The regularization coefficient for leaf score, also known as lambda.\n",
    "        gamma: The regularization coefficient for number of tree nodes, also know as gamma.\n",
    "        rf: rf*m is the size of random subset of features, from which we select the best decision rule.\n",
    "        num_trees: Number of trees.\n",
    "    '''\n",
    "    def __init__(self,\n",
    "        n_threads = None, loss = 'mse', max_depth = 3, min_sample_split = 10, lamda = 1, gamma = 0, rf = 0.99, num_trees = 100, gradient_seed = 'zeros'):\n",
    "        \n",
    "        self.n_threads = n_threads\n",
    "        self.loss = loss\n",
    "        self.max_depth = max_depth\n",
    "        self.min_sample_split = min_sample_split\n",
    "        self.lamda = lamda\n",
    "        self.gamma = gamma\n",
    "        self.rf = rf\n",
    "        self.num_trees = num_trees\n",
    "        self.trees_in_forest = []\n",
    "        self.gradient_seed = gradient_seed\n",
    "        if loss == \"mse\": self.loss_fn = leastsquare()\n",
    "        else: self.loss_fn = logistic()\n",
    "\n",
    "    def fit(self, train, target):\n",
    "        # train is n x m 2d numpy array\n",
    "        # target is n-dim 1d array\n",
    "        #TODO\n",
    "        n,m = train.shape\n",
    "        for _ in range(self.num_trees):\n",
    "            indices = np.random.choice(np.arange(n), n, replace=True)\n",
    "            train_bt = train[indices, :]\n",
    "            target_bt = target[indices]\n",
    "            tree = Tree(n_threads=self.n_threads, max_depth=self.max_depth, min_sample_split=self.min_sample_split, rf=self.rf,\n",
    "                 lamda=self.lamda, gamma=self.gamma, )\n",
    "\n",
    "            if self.gradient_seed == 'zeros': y_0_hat = np.zeros((n,))\n",
    "            elif self.gradient_seed == 'mean': y_0_hat = np.ones((n,)) * target.mean()\n",
    "\n",
    "            gradients = self.loss_fn.g(target_bt, y_0_hat)\n",
    "            hessians = self.loss_fn.h(target_bt, y_0_hat)\n",
    "            tree.fit(train_bt, gradients, hessians)\n",
    "\n",
    "            self.trees_in_forest.append(tree)\n",
    "        return self\n",
    "\n",
    "    def predict(self, test):\n",
    "        #TODO\n",
    "        preds_from_trees = []\n",
    "        for tree in self.trees_in_forest:\n",
    "            score = tree.predict(test)\n",
    "            preds_from_trees.append(self.loss_fn.pred(score))\n",
    "        preds_from_trees = np.array(preds_from_trees)\n",
    "        if self.loss == 'mse':\n",
    "            return np.mean(np.array(preds_from_trees), axis=0)\n",
    "        else:\n",
    "            return (preds_from_trees.sum(axis=0) > (self.num_trees//2)).astype('int') # majority vote\n",
    "     #   return self.loss.pred(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: class of GBDT\n",
    "class GBDT(object):\n",
    "    '''\n",
    "    Class of gradient boosting decision tree (GBDT)\n",
    "    \n",
    "    Parameters:\n",
    "        n_threads: The number of threads used for fitting and predicting.\n",
    "        loss: Loss function for gradient boosting.\n",
    "            'mse' for regression task and 'log' for classfication task.\n",
    "            A child class of the loss class could be passed to implement customized loss.\n",
    "        max_depth: The maximum depth D_max of a tree.\n",
    "        min_sample_split: The minimum number of samples required to further split a node.\n",
    "        lamda: The regularization coefficient for leaf score, also known as lambda.\n",
    "        gamma: The regularization coefficient for number of tree nodes, also know as gamma.\n",
    "        learning_rate: The learning rate eta of GBDT.\n",
    "        num_trees: Number of trees.\n",
    "    '''\n",
    "    def __init__(self,\n",
    "        n_threads = None, loss = 'mse',\n",
    "        max_depth = 3, min_sample_split = 10, \n",
    "        lamda = 1, gamma = 0,\n",
    "        learning_rate = 0.1, num_trees = 100):\n",
    "        \n",
    "        self.n_threads = n_threads\n",
    "        self.loss = loss\n",
    "        self.max_depth = max_depth\n",
    "        self.min_sample_split = min_sample_split\n",
    "        self.lamda = lamda\n",
    "        self.gamma = gamma\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_trees = num_trees\n",
    "        self.trees = []\n",
    "        if loss == \"mse\": self.loss_fn = leastsquare()\n",
    "        else: self.loss_fn = logistic()\n",
    "\n",
    "    def fit(self, train, target, bootstrap = True):\n",
    "        # train is n x m 2d numpy array\n",
    "        # target is n-dim 1d array\n",
    "        #TODO\n",
    "        n,m = train.shape\n",
    "        self.y_train_mean = target.mean()\n",
    "        y_hat = np.ones(n) * target.mean()\n",
    "        for _ in range(self.num_trees):\n",
    "\n",
    "            indices = np.random.choice(np.arange(n), n, replace=True)\n",
    "            train_bt = train[indices, :]\n",
    "            target_bt = target[indices]\n",
    "            tree = Tree(n_threads=self.n_threads, max_depth=self.max_depth, min_sample_split=self.min_sample_split, rf=1,\n",
    "                 lamda=self.lamda, gamma=self.gamma, )\n",
    "\n",
    "            gradients = self.loss_fn.g(target_bt, y_hat)\n",
    "            hessians = self.loss_fn.h(target_bt, y_hat)\n",
    "            tree.fit(train_bt, gradients, hessians)\n",
    "            self.trees.append(tree)\n",
    "            y_hat = y_hat + self.learning_rate * self.predict(train)\n",
    "        return self\n",
    "\n",
    "    def predict(self, test):\n",
    "        #TODO\n",
    "        n,m = test.shape\n",
    "        preds = np.zeros((n, self.num_trees))\n",
    "        y_hat_0 = np.ones(n) * self.y_train_mean\n",
    "        preds[:,0] = y_hat_0 + self.learning_rate * np.array(self.trees[0].predict(test))\n",
    "        for tree_idx in np.arange(1, len(self.trees)):\n",
    "            tree = self.trees[tree_idx]\n",
    "            score = np.array(tree.predict(test))\n",
    "            preds[:, tree_idx] = preds[:, tree_idx-1] + self.learning_rate * score\n",
    "\n",
    "        return self.loss_fn.pred(preds.mean(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: class of a node on a tree\n",
    "class TreeNode(object):\n",
    "    '''\n",
    "    Data structure that are used for storing a node on a tree.\n",
    "    \n",
    "    A tree is presented by a set of nested TreeNodes,\n",
    "    with one TreeNode pointing two child TreeNodes,\n",
    "    until a tree leaf is reached.\n",
    "    \n",
    "    A node on a tree can be either a leaf node or a non-leaf node.\n",
    "    '''\n",
    "    \n",
    "    #TODO\n",
    "    def __init__(self, split_feature=None, split_threshold=None, left_child=None, right_child=None, gain=None, weight= None):\n",
    "        self.split_feature = split_feature\n",
    "        self.split_threshold = split_threshold\n",
    "        self.left_child = left_child\n",
    "        self.right_child = right_child\n",
    "        self.gain = gain\n",
    "        self.weight = weight \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: class of single tree\n",
    "class Tree(object):\n",
    "    '''\n",
    "    Class of a single decision tree in GBDT\n",
    "\n",
    "    Parameters:\n",
    "        n_threads: The number of threads used for fitting and predicting.\n",
    "        max_depth: The maximum depth of the tree.\n",
    "        min_sample_split: The minimum number of samples required to further split a node.\n",
    "        lamda: The regularization coefficient for leaf prediction, also known as lambda.\n",
    "        gamma: The regularization coefficient for number of TreeNode, also know as gamma.\n",
    "        rf: rf*m is the size of random subset of features, from which we select the best decision rule,\n",
    "            rf = 0 means we are training a GBDT.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, n_threads = None, \n",
    "                 max_depth = 3, min_sample_split = 10,\n",
    "                 lamda = 1, gamma = 0, rf = 0):\n",
    "        self.n_threads = n_threads\n",
    "        self.max_depth = max_depth\n",
    "        self.min_sample_split = min_sample_split\n",
    "        self.lamda = lamda\n",
    "        self.gamma = gamma\n",
    "        self.rf = 0\n",
    "        self.int_member = 0\n",
    "\n",
    "    def fit(self, train, g, h):\n",
    "        '''\n",
    "        train is the training data matrix, and must be numpy array (an n_train x m matrix).\n",
    "        g and h are gradient and hessian respectively.\n",
    "        '''\n",
    "        #TODO\n",
    "        #self.root = self.construct_tree(train, g, h)\n",
    "        self.root = self.construct_tree(train, g, h)\n",
    "\n",
    "        #return self\n",
    "\n",
    "    def predict(self,test):\n",
    "        '''\n",
    "        test is the test data matrix, and must be numpy arrays (an n_test x m matrix).\n",
    "        Return predictions (scores) as an array.\n",
    "        '''\n",
    "        #TODO\n",
    "        preds = [self.make_prediction(x, self.root) for x in test]\n",
    "        return preds\n",
    "    def make_prediction(self, x, tree):\n",
    "        ''' function to predict a single data point '''\n",
    "        if tree.weight != None: return tree.weight\n",
    "        feature_val = x[tree.split_feature]\n",
    "        if feature_val <= tree.split_threshold:\n",
    "            return self.make_prediction(x, tree.left_child)\n",
    "        else:\n",
    "            return self.make_prediction(x, tree.right_child) \n",
    "\n",
    "    def construct_tree(self, train, g, h, curr_depth = 0):\n",
    "        '''\n",
    "        Tree construction, which is recursively used to grow a tree.\n",
    "        First we should check if we should stop further splitting.\n",
    "        \n",
    "        The stopping conditions include:\n",
    "            1. tree reaches max_depth $d_{max}$\n",
    "            2. The number of sample points at current node is less than min_sample_split, i.e., $n_{min}$\n",
    "            3. gain <= 0\n",
    "        '''\n",
    "        #TODO\n",
    "        n, m = train.shape\n",
    "        if n >= self.min_sample_split and self.max_depth >= curr_depth:\n",
    "            best_feature_idx, best_threshold, gain, dataset_left, dataset_right, g_l, g_r, h_l, h_r = self.find_best_decision_rule(train, g, h)\n",
    "            if gain > 0:\n",
    "                left_subtree = self.construct_tree(dataset_left, g_l, h_l, curr_depth+1)\n",
    "                right_subtree = self.construct_tree(dataset_right, g_r, h_r, curr_depth+1)\n",
    "                return TreeNode(split_feature = best_feature_idx, split_threshold = best_threshold,\n",
    "                                left_child = left_subtree, right_child = right_subtree, gain = gain)\n",
    "\n",
    "        # if we are at leaf\n",
    "        weight = -np.sum(g) / (np.sum(h) + self.lamda)\n",
    "        return TreeNode(weight=weight)\n",
    "        \n",
    "        \n",
    "    def find_best_decision_rule(self, train, g, h):\n",
    "        '''\n",
    "        Return the best decision rule [feature, treshold], i.e., $(p_j, \\tau_j)$ on a node j, \n",
    "        train is the training data assigned to node j\n",
    "        g and h are the corresponding 1st and 2nd derivatives for each data point in train\n",
    "        g and h should be vectors of the same length as the number of data points in train\n",
    "        \n",
    "        for each feature, we find the best threshold by find_threshold(),\n",
    "        a [threshold, best_gain] list is returned for each feature.\n",
    "        Then we select the feature with the largest best_gain,\n",
    "        and return the best decision rule [feature, treshold] together with its gain.\n",
    "        '''\n",
    "        #TODO\n",
    "        n, m = train.shape\n",
    "        if self.rf:\n",
    "            feat_indices = np.random.choice(np.arange(m), size=round(self.rf*m))\n",
    "            train = train[:,feat_indices]\n",
    "\n",
    "        n, m = train.shape\n",
    "        best_gain = -float('inf')\n",
    "        best_threshold, best_left, best_right, best_feat_idx = None, None, None, None\n",
    "        best_g_l, best_g_r, best_h_l, best_h_r = None, None, None, None\n",
    "        for feat_idx in range(m):\n",
    "            feature_values = train[:,feat_idx]\n",
    "            for cand_thresh in np.unique(feature_values):\n",
    "                dataset_left, dataset_right, g_l, g_r, h_l, h_r = self.split(train, g, h, feat_idx, cand_thresh)\n",
    "                if len(dataset_left) > 0 and len(dataset_right) > 0:\n",
    "                    G_L, H_L = np.sum(g_l), np.sum(h_l)\n",
    "                    G_R, H_R = np.sum(g_r), np.sum(h_r)\n",
    "                    term1 = G_L**2/(H_L + self.lamda)\n",
    "                    term2 = G_R**2/(H_R + self.lamda)\n",
    "                    term3 = (G_L + G_R)**2/ (H_L + H_R + self.lamda)\n",
    "                    gain = 0.5*(term1 + term2 - term3) - self.gamma\n",
    "                    if gain > best_gain:\n",
    "                        best_gain = gain\n",
    "                        best_threshold = cand_thresh\n",
    "                        best_feat_idx = feat_idx\n",
    "                        best_left, best_right = dataset_left, dataset_right\n",
    "                        best_g_l, best_g_r, best_h_l, best_h_r = g_l, g_r, h_l, h_r\n",
    "\n",
    "        return best_feat_idx, best_threshold, best_gain, best_left, best_right, best_g_l, best_g_r, best_h_l, best_h_r\n",
    "        #return feature, threshold, gain\n",
    "    \n",
    "    #def find_threshold(self, g, h, train):\n",
    "     #   '''\n",
    "      #  Given a particular feature $p_j$,\n",
    "       # return the best split threshold $\\tau_j$ together with the gain that is achieved.\n",
    "       # '''\n",
    "        #TODO               \n",
    "        #return [threshold, best_gain]\n",
    "    def print_tree(self, tree=None, indent=\" \"):\n",
    "        ''' function to print the tree '''\n",
    "        if not tree: tree = self.root\n",
    "        if tree.weight is not None:\n",
    "            print(tree.weight)\n",
    "        else:\n",
    "            print(\"X_\"+str(tree.split_feature), \"<=\", tree.split_threshold, \"?\", tree.gain)\n",
    "            print(\"%sleft:\" % (indent), end=\"\")\n",
    "            self.print_tree(tree.left_child, indent + indent)\n",
    "            print(\"%sright:\" % (indent), end=\"\")\n",
    "            self.print_tree(tree.right_child, indent + indent)\n",
    "\n",
    "    def split(self, train, g, h, best_feature_idx, best_threshold):\n",
    "        n = train.shape[0]\n",
    "        left_mask = np.arange(n)[train[:,best_feature_idx] <= best_threshold]\n",
    "        right_mask = np.arange(n)[train[:,best_feature_idx] > best_threshold]\n",
    "        return train[left_mask], train[right_mask], g[left_mask], g[right_mask], h[left_mask], h[right_mask]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Evaluation functions (you can use code from previous homeworks)\n",
    "\n",
    "# RMSE\n",
    "def root_mean_square_error(pred, y):\n",
    "    #TODO\n",
    "    return np.sqrt(np.mean((pred-y)**2))\n",
    "\n",
    "# precision\n",
    "def accuracy(pred, y):\n",
    "    #TODO\n",
    "    return (pred == y).sum() / len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boston house price dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harsheetav/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows::\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and::\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# TODO: GBDT regression on boston house price dataset\n",
    "\n",
    "# load data\n",
    "from sklearn import datasets\n",
    "boston = datasets.load_boston()\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "\n",
    "# train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "performance of a single decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE using Random Forest, zero gradient-seed:  3.142174806201666\n",
      "Test RMSE using Random Forest, zero gradient-seed:  4.151151506212169\n"
     ]
    }
   ],
   "source": [
    "rf = RF(gradient_seed=\"zeros\")\n",
    "rf.fit(X_train, y_train)\n",
    "train_pred = rf.predict(X_train)\n",
    "train_rmse_RF_zeros = root_mean_square_error(train_pred, y_train)\n",
    "print(\"Train RMSE using Random Forest, zero gradient-seed: \", train_rmse_RF_zeros)\n",
    "\n",
    "test_pred = rf.predict(X_test)\n",
    "test_rmse_RF_zeros = root_mean_square_error(test_pred, y_test)\n",
    "print(\"Test RMSE using Random Forest, zero gradient-seed: \", test_rmse_RF_zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lamdas = np.arange(11)\n",
    "# gammas = np.linspace(0,1, 10)\n",
    "# learning_rates = np.linspace(0.1,1, 10)\n",
    "max_depths = np.arange(2,11)\n",
    "min_sample_splits = np.arange(1,51,10)\n",
    "num_trees_list = np.arange(5,25,10)\n",
    "expts = pd.DataFrame(columns=['max_depth','min_sample_split','num_trees','train_rmse','test_rmse'], dtype=object)\n",
    "for max_depth in max_depths:\n",
    "    for min_sample_split in min_sample_splits:\n",
    "        for num_trees in num_trees_list:\n",
    "            gbdt = GBDT(num_trees=num_trees, gamma=20, min_sample_split=min_sample_split, max_depth=max_depth)\n",
    "            gbdt.fit(X_train, y_train)\n",
    "            train_pred = gbdt.predict(X_train)\n",
    "            train_rmse_GBDT_zeros = root_mean_square_error(train_pred, y_train)\n",
    "            test_pred = gbdt.predict(X_test)\n",
    "            test_rmse_GBDT_zeros = root_mean_square_error(test_pred, y_test)\n",
    "            expts.loc[len(expts.index)] = [max_depth, min_sample_split, num_trees, train_rmse_GBDT_zeros, test_rmse_GBDT_zeros]\n",
    "            param_str = f\"{max_depth}, {min_sample_split}, {num_trees}\"\n",
    " # print(f\"{param_str}: {train_rmse_GBDT_zeros}, {test_rmse_GBDT_zeros}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "regression on boston dataset using least squares and ridge, HW2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set RMSE, least squares regression model :  5.209217510530889\n",
      "Train set RMSE, least squares regression model :  4.820626531838222\n",
      "\n",
      "Test set RMSE, Ridge model :  5.187846945948004\n",
      "Train set RMSE, Ridge model :  4.837923187275855\n"
     ]
    }
   ],
   "source": [
    "def least_square(X, y):\n",
    "    theta = np.linalg.inv(X.T @ X) @ (X.T @ y)\n",
    "    return theta\n",
    "def ridge_reg(X, y, eta):\n",
    "    n = X.shape[1]\n",
    "    theta = np.linalg.inv((X.T @ X) + eta*np.eye(n)) @ (X.T @ y)\n",
    "    return theta\n",
    "def pred_fn(X, theta):\n",
    "    pred = (X @ theta)\n",
    "    return pred\n",
    "theta = least_square(X_train, y_train)\n",
    "theta_r = ridge_reg(X_train, y_train, 15.0)\n",
    "pred_test_vanilla = pred_fn(X_test, theta)\n",
    "pred_test_r = pred_fn(X_test, theta_r)\n",
    "rmse_test_vanilla = root_mean_square_error(pred_test_vanilla, y_test)\n",
    "rmse_test_r = root_mean_square_error(pred_test_r, y_test)\n",
    "pred_train_vanilla = pred_fn(X_train, theta)\n",
    "pred_train_r = pred_fn(X_train, theta_r)\n",
    "rmse_train_vanilla = root_mean_square_error(pred_train_vanilla, y_train)\n",
    "rmse_train_r = root_mean_square_error(pred_train_r, y_train)\n",
    "print(\"Test set RMSE, least squares regression model : \", rmse_test_vanilla)\n",
    "print(\"Train set RMSE, least squares regression model : \", rmse_train_vanilla)\n",
    "print(\"\\nTest set RMSE, Ridge model : \", rmse_test_r)\n",
    "print(\"Train set RMSE, Ridge model : \", rmse_train_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Set the labels for the bar chart\n",
    "alg_names = ['Least squares', 'Ridge', 'Random Forest\\n(gradient-seed: 0)', 'GBDT\\n']\n",
    "\n",
    "# Round the training and test RMSE values to 3 decimal places\n",
    "train_rmse = [np.round(x, 3) for x in [rmse_train_vanilla, rmse_train_r, train_rmse_RF_zeros, train_rmse_GBDT]]\n",
    "test_rmse = [np.round(x, 3) for x in [rmse_test_vanilla, rmse_test_r, test_rmse_RF_zeros, test_rmse_GBDT]]\n",
    "\n",
    "# Create an array of x-coordinates for the bar chart\n",
    "x_pos = np.arange(len(alg_names))\n",
    "\n",
    "# Set the colors for the bars\n",
    "train_color = 'green'\n",
    "test_color = 'red'\n",
    "\n",
    "# Create a new figure with a specific size and DPI\n",
    "fig, ax = plt.subplots(figsize=(8, 4), dpi=100)\n",
    "\n",
    "# Set the width of the bars\n",
    "bar_width = 0.35\n",
    "\n",
    "# Create the bars for the training and test RMSE values\n",
    "train_bars = ax.bar(x_pos - bar_width/2, train_rmse, width=bar_width, label='Train', color=train_color)\n",
    "test_bars = ax.bar(x_pos + bar_width/2, test_rmse, width=bar_width, label='Test', color=test_color)\n",
    "\n",
    "# Set the axis labels and title\n",
    "ax.set_ylabel('RMSE')\n",
    "ax.set_title('RMSE on Boston Housing Dataset')\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(alg_names)\n",
    "ax.legend()\n",
    "\n",
    "# Add labels to the bars\n",
    "ax.bar_label(train_bars, padding=3)\n",
    "ax.bar_label(test_bars, padding=3)\n",
    "\n",
    "# Set the y-axis limits\n",
    "ax.set_ylim((0, 6))\n",
    "\n",
    "# Adjust the layout and display the figure\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credit-g dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# TODO: GBDT classification on credit-g dataset\n",
    "\n",
    "# load data\n",
    "from sklearn.datasets import fetch_openml\n",
    "X, y = fetch_openml('credit-g', version=1, return_X_y=True, data_home='credit/')\n",
    "y = np.array(list(map(lambda x: 1 if x == 'good' else 0, y)))\n",
    "\n",
    "# train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=8)\n",
    "#X_train = pd.get_dummies(X_train)\n",
    "#X_test = pd.get_dummies(X_test)\n",
    "print(type(X_train))   # should be numpy.ndarray\n",
    "print(type(y_train))   # should be numpy.ndarray\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Breast cancer dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: GBDT classification on breast cancer dataset\n",
    "\n",
    "# load data\n",
    "from sklearn import datasets\n",
    "breast_cancer = datasets.load_breast_cancer()\n",
    "X = breast_cancer.data\n",
    "y = breast_cancer.target\n",
    "\n",
    "# train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy using Random Forest, zeros gradient-seed:  0.992462311557789\n",
      "Test accuracy using Random Forest, zeros gradient-seed:  0.9649122807017544\n"
     ]
    }
   ],
   "source": [
    "rf = RF(loss='log', gradient_seed=\"zeros\")\n",
    "rf.fit(X_train, y_train)\n",
    "train_pred = rf.predict(X_train)\n",
    "train_acc_RF_zeros = accuracy(train_pred, y_train)\n",
    "print(\"Train accuracy using Random Forest, zeros gradient-seed: \", train_acc_RF_zeros)\n",
    "test_pred = rf.predict(X_test)\n",
    "test_acc_RF_zeros = accuracy(test_pred, y_test)\n",
    "print(\"Test accuracy using Random Forest, zeros gradient-seed: \", test_acc_RF_zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy using GBDT:  0.9748743718592965\n",
      "Test accuracy using GBDT:  0.9415204678362573\n"
     ]
    }
   ],
   "source": [
    "gbdt = GBDT(num_trees=15, gamma=1, min_sample_split=31, max_depth=8, loss=\"log\")\n",
    "gbdt.fit(X_train, y_train)\n",
    "train_pred = gbdt.predict(X_train)\n",
    "train_acc_GBDT = accuracy(train_pred, y_train)\n",
    "print(\"Train accuracy using GBDT: \", train_acc_GBDT)\n",
    "test_pred = gbdt.predict(X_test)\n",
    "test_acc_GBDT = accuracy(test_pred, y_test)\n",
    "print(\"Test accuracy using GBDT: \", test_acc_GBDT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
